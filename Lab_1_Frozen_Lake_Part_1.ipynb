{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "auburn-pittsburgh",
   "metadata": {
    "id": "auburn-pittsburgh",
    "tags": []
   },
   "source": [
    "# Lab 1 - FrozenLake MDP: Part 1\n",
    "# Assignment\n",
    "\n",
    "- In this assignment you will learn how to tackle problems with limited state spaces.\n",
    "- In particular we consider the FrozenLake MDP problem.\n",
    "\n",
    "# Outline\n",
    "\n",
    "- Part 0 introduces us to [gym](https://gym.openai.com/), an environment that allows us to test our reinforcement learning algorithms in various problems\n",
    "- In Part 1, you will implement a policy iteration algorithm (HW1)\n",
    "- In Part 2, you will implement Q-Learning and SARSA (in next homework HW2) \n",
    "\n",
    "# Deliverable\n",
    "\n",
    "Regarding the Lab:\n",
    "\n",
    "- Make sure your code runs from top to bottom without any errors.\n",
    "- Your submitted Notebook must contain saved outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "flush-grove",
   "metadata": {
    "id": "flush-grove"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# You will need numpy and gym. You can try running the following lines to install them\n",
    "# The assignment is tested on Python3.8 so in case you are having installation issues you might \n",
    "# want to try installing that version. \n",
    "\n",
    "# !{os.sys.executable} -m pip install numpy\n",
    "# !{os.sys.executable} -m pip install gym\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-dancing",
   "metadata": {
    "id": "medium-dancing"
   },
   "source": [
    "# Part 0 - Introduction to Gym\n",
    "- We look at [FrozenLake-v0 environment](https://gym.openai.com/envs/FrozenLake-v0/) in gym. \n",
    "- You don't need to write any code for this part\n",
    "- you should still understand the code to help you solve Part 1 and Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "considerable-swing",
   "metadata": {
    "id": "considerable-swing"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of States 16, Number of Actions 4\n",
      "Reward range (0, 1)\n"
     ]
    }
   ],
   "source": [
    "# Import the environment we will use in this assignment\n",
    "# env=gym.make('FrozenLake-v0') \n",
    "env=gym.make('FrozenLake-v1',is_slippery=True)\n",
    "# Show the model\n",
    "print(f\"Number of States {env.nS}, Number of Actions {env.nA}\")\n",
    "print(f\"Reward range {env.reward_range}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "severe-conjunction",
   "metadata": {
    "id": "severe-conjunction"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset() # reset the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "endless-cassette",
   "metadata": {
    "id": "endless-cassette"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# visualize the current state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "naughty-bread",
   "metadata": {
    "id": "naughty-bread"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Render State after 25 slots\n",
      "  (Down)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Reached terminal state? True\n"
     ]
    }
   ],
   "source": [
    "# run a policy that chooses actions randomly \n",
    "env.reset()\n",
    "n = 25\n",
    "for i in range(n):\n",
    "    a = env.action_space.sample() # Sample Random Action\n",
    "    state, reward, finished, _ = env.step(a)\n",
    "    if finished: break\n",
    "        \n",
    "print(f'Render State after {n} slots')\n",
    "env.render()\n",
    "print(f'Reached terminal state? {finished}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "front-nomination",
   "metadata": {
    "id": "front-nomination"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset() # Let's reset the state again\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-china",
   "metadata": {
    "id": "official-china"
   },
   "source": [
    "# Part 1 - MDP and Planning: Implement Policy Iteration \n",
    "- In this part we will focus on methods that assume knowledge of the enivonment dynamics, in partucular you will implement Policy Iteration. \n",
    "- The environment model can be obtained through `env.P`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "spanish-feeding",
   "metadata": {
    "id": "spanish-feeding"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 11, 0, True)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No need to change anything here. Try to understand what happens \n",
    "\n",
    "# let's look at a random state-action pair and observe its transition characteristics\n",
    "# you can re-run this cell to get a different state-action pair\n",
    "random_state  = env.observation_space.sample()\n",
    "random_action = env.action_space.sample()\n",
    "# returns a list of tuples (probability,newstate,reward,is_terminal_state)\n",
    "env.P[random_state][random_action] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lucky-place",
   "metadata": {
    "id": "lucky-place"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5, 7, 11, 12, 15}\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# YOUR CODE HERE #\n",
    "# Print all the terminal states in the environment.\n",
    "# you can use env.P\n",
    "\n",
    "terminal_states = set()\n",
    "for state in range(env.nS):\n",
    "    for action in range(env.nA):\n",
    "        for possible_state in env.P[state][action]:\n",
    "            if possible_state[3]:\n",
    "                terminal_states.add(possible_state[1])\n",
    "\n",
    "print(terminal_states)\n",
    "        \n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rocky-danger",
   "metadata": {
    "id": "rocky-danger"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Verify your solution (look at the positions where final states are)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-pendant",
   "metadata": {
    "id": "protective-pendant"
   },
   "source": [
    "\n",
    "### Step A: Implement Policy Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "forty-offering",
   "metadata": {
    "id": "forty-offering"
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"Evaluate the value function from a given policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    policy: np.array[nS]\n",
    "        The policy to evaluate. Maps states to actions.\n",
    "    tol: float\n",
    "        Terminate policy evaluation when\n",
    "            max |value_function(s) - prev_value_function(s)| < tol\n",
    "    Returns\n",
    "    -------\n",
    "    value_function: np.ndarray[nS]\n",
    "        The value function of the given policy, where value_function[s] is\n",
    "        the value of state s\n",
    "    \"\"\"\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        diff = 0\n",
    "        for s in range(nS):\n",
    "                        \n",
    "            v = value_function[s] \n",
    "            value_function[s] = sum([p*(r+gamma*value_function[n]) for p,n,r,_ in env.P[s][policy[s]]])\n",
    "            diff = max(diff,abs(v-value_function[s]))\n",
    "\n",
    "        if diff < tol:\n",
    "            break\n",
    "\n",
    "    ############################\n",
    "    return value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-property",
   "metadata": {
    "id": "stunning-property"
   },
   "source": [
    "#### Evaluate random policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "naked-tooth",
   "metadata": {
    "id": "naked-tooth"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Policy 0 ------------------------------\n",
      "[0.02592173 0.02506689 0.03320955 0.         0.03734309 0.\n",
      " 0.07764633 0.         0.08814752 0.16909485 0.22570486 0.\n",
      " 0.         0.24994353 0.5832928  0.        ]\n",
      "-------- Policy 1 ------------------------------\n",
      "[0.00443043 0.         0.         0.         0.01137315 0.\n",
      " 0.         0.         0.03405614 0.08036977 0.16390021 0.\n",
      " 0.         0.23405313 0.5464036  0.        ]\n",
      "-------- Policy 2 ------------------------------\n",
      "[0.00419281 0.0016827  0.02861212 0.01210817 0.00928153 0.\n",
      " 0.05569844 0.         0.02710647 0.06383039 0.18583226 0.\n",
      " 0.         0.26526162 0.55576615 0.        ]\n",
      "-------- Policy 3 ------------------------------\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "-------- Policy 4 ------------------------------\n",
      "[0.00637233 0.00258793 0.00505027 0.00324463 0.01382438 0.\n",
      " 0.00965438 0.         0.04034093 0.08116614 0.02724616 0.\n",
      " 0.         0.243703   0.4878335  0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Test your policy_evaluation on 5 randomly generated deterministic policies\n",
    "# print the value function of the policies\n",
    "\n",
    "############################\n",
    "# YOUR CODE HERE #\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    random_policy = np.random.randint(env.nA,size=env.nS)\n",
    "    print(f'-------- Policy {i}','-'*30)\n",
    "    print(policy_evaluation(env.P, env.nS, env.nA, random_policy))\n",
    "\n",
    "############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-evaluation",
   "metadata": {
    "id": "ongoing-evaluation"
   },
   "source": [
    "### Step B: Implement Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cooperative-dressing",
   "metadata": {
    "id": "cooperative-dressing"
   },
   "outputs": [],
   "source": [
    "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Given the value function from policy improve the policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    value_from_policy: np.ndarray\n",
    "        The value calculated from the policy\n",
    "    policy: np.array\n",
    "        The previous policy.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_policy: np.ndarray[nS]\n",
    "        An array of integers. Each integer is the optimal action to take\n",
    "        in that state according to the environment dynamics and the\n",
    "        given value function.\n",
    "    \"\"\"\n",
    "    new_policy = np.zeros(nS, dtype='int')\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    \n",
    "    for s in range(nS):\n",
    "        \n",
    "#         if s in terminal_states:\n",
    "#                 continue\n",
    "\n",
    "        new_policy[s] = np.argmax([sum([p*(r+gamma*value_from_policy[n]) for p,n,r,_ in env.P[s][a]])\n",
    "                                 for a in range(nA)])\n",
    "        \n",
    "#         old_action = policy[s]\n",
    "#         new_action = policy[s]\n",
    "#         max_value = float('-inf')\n",
    "#         for a in range(nA):\n",
    "#             new_value = sum([p*(r+gamma*value_function[n]) for p,n,r,_ in env.P[s][a]])\n",
    "#             if new_value > max_value:\n",
    "#                 max_value = new_value\n",
    "#                 new_action = a\n",
    "#         new_policy[s] = new_action\n",
    "            \n",
    "    ############################\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "behavioral-penetration",
   "metadata": {
    "id": "behavioral-penetration"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Policy 0 ------------------------------\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.14251    0.\n",
      " 0.         0.         0.47584333 0.        ]\n",
      "-------> Policy 0 IMPROVED ---------------------\n",
      "[0.         0.         0.03494066 0.01474878 0.         0.\n",
      " 0.08308461 0.         0.         0.14678545 0.2427983  0.\n",
      " 0.         0.24802032 0.5800101  0.        ]\n",
      "-------- Policy 1 ------------------------------\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "-------> Policy 1 IMPROVED ---------------------\n",
      "[0.         0.         0.02279889 0.00950109 0.         0.\n",
      " 0.05453427 0.         0.         0.         0.15918618 0.\n",
      " 0.         0.         0.47615923 0.        ]\n",
      "-------- Policy 2 ------------------------------\n",
      "[0.00483051 0.00936909 0.0265529  0.         0.00239941 0.\n",
      " 0.0793711  0.         0.0009961  0.17325761 0.23839423 0.\n",
      " 0.         0.3402466  0.62182426 0.        ]\n",
      "-------> Policy 2 IMPROVED ---------------------\n",
      "[0.02581715 0.03683704 0.0612966  0.02617257 0.06662355 0.\n",
      " 0.10691814 0.         0.13116705 0.24057771 0.2953762  0.\n",
      " 0.         0.37599919 0.63726053 0.        ]\n",
      "-------- Policy 3 ------------------------------\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "-------> Policy 3 IMPROVED ---------------------\n",
      "[0.         0.         0.02279889 0.00950109 0.         0.\n",
      " 0.05453427 0.         0.         0.         0.15918618 0.\n",
      " 0.         0.         0.47615923 0.        ]\n",
      "-------- Policy 4 ------------------------------\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "-------> Policy 4 IMPROVED ---------------------\n",
      "[0.         0.         0.02279889 0.00950109 0.         0.\n",
      " 0.05453427 0.         0.         0.         0.15918618 0.\n",
      " 0.         0.         0.47615923 0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Print the value before and after policy improvements for 5 randomly generated policies\n",
    "\n",
    "############################\n",
    "# YOUR CODE HERE #\n",
    "\n",
    "for i in range(5):\n",
    "    random_policy = np.random.randint(env.nA,size=env.nS)\n",
    "    print(f'-------- Policy {i}','-'*30)\n",
    "#     print(random_policy)\n",
    "    value_function = policy_evaluation(env.P, env.nS, env.nA, random_policy)\n",
    "#     print(f'-------- Value {i}','-'*30)\n",
    "    print(value_function)\n",
    "    \n",
    "    print(f'-------> Policy {i} IMPROVED','-'*21)\n",
    "    new_policy = policy_improvement(env.P, env.nS, env.nA, value_function, random_policy)\n",
    "#     print(new_policy)\n",
    "#     print(f'-------- Value {i} IMPROVED','-'*21)\n",
    "    print(policy_evaluation(env.P, env.nS, env.nA, new_policy))\n",
    "\n",
    "############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-tragedy",
   "metadata": {
    "id": "independent-tragedy"
   },
   "source": [
    "### Step C: Implement Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ranking-template",
   "metadata": {
    "id": "ranking-template"
   },
   "outputs": [],
   "source": [
    "def policy_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "    \"\"\" \n",
    "    Run policy iteration for dynamics of P.\n",
    "\n",
    "    You should use your methods: policy_evaluation() and policy_improvement() here\n",
    "\n",
    "    Parameters: \n",
    "    P, nS, nA, gamma: defined at beginning of file\n",
    "    tolerance:        tolerance parameter used in policy_evaluation()\n",
    "    \n",
    "    Returns: \n",
    "    value_function: np.ndarray[nS]\n",
    "    policy:         np.ndarray[nS]\n",
    "    \"\"\"\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        value_function = policy_evaluation(P, nS, nA, policy, gamma, tol)\n",
    "        new_policy = policy_improvement(P, nS, nA, value_function, policy, gamma)\n",
    "        \n",
    "        if np.array_equal(policy,new_policy): #or np.array_equal(new_value_function,value_function):\n",
    "            break\n",
    "            \n",
    "        policy = new_policy\n",
    "        \n",
    "    ############################\n",
    "    return value_function, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-cookie",
   "metadata": {
    "id": "individual-cookie"
   },
   "source": [
    "#### Call your function for gamma=0.9 and gamma=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "subsequent-tonight",
   "metadata": {
    "id": "subsequent-tonight"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.06476289, 0.05841569, 0.07252933, 0.05378964, 0.08867795,\n",
       "        0.        , 0.11137269, 0.        , 0.14325408, 0.24628655,\n",
       "        0.29886952, 0.        , 0.        , 0.37915415, 0.63865133,\n",
       "        0.        ]),\n",
       " array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_pi_s, p_pi_s = policy_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
    "V_pi_s, p_pi_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "military-presence",
   "metadata": {
    "id": "military-presence"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00069547, 0.00139264, 0.00562142, 0.00171616, 0.00280064,\n",
       "        0.        , 0.02169924, 0.        , 0.01214942, 0.04755381,\n",
       "        0.1032758 , 0.        , 0.        , 0.12348492, 0.44745551,\n",
       "        0.        ]),\n",
       " array([1, 3, 2, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_pi_f, p_pi_f = policy_iteration(env.P, env.nS, env.nA, gamma=0.6, tol=1e-3)\n",
    "V_pi_f, p_pi_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-unknown",
   "metadata": {
    "id": "fifth-unknown"
   },
   "source": [
    "#### What do you observe in terms of impact of gamma on the actions taken by the policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GGe3wwuEoKye",
   "metadata": {
    "id": "GGe3wwuEoKye"
   },
   "source": [
    "*YOUR ANSWER*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_1_Frozen_Lake_Part_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
