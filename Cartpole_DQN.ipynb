{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCn5gK-8Qhtz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#  Instruction\n",
    "\n",
    "In this notebook, we will learn how to implement DQN using Tensorflow for the [Cartpole environment in OpenAI gym](https://gym.openai.com/envs/CartPole-v0/). You are given a basic skeleton but you need to complete the code where appropriate to solve the cartpole problem.\n",
    "\n",
    "You are free to tweak the code at any part. Your are also free to tweak the hyper-parameters to improve the performance of the agent. At the end you have to evaluate the performance of the agent on 100 independent episodes on the environment and print out the average testing performance.\n",
    "\n",
    "Make sure that your final submission is a notebook that can be run from beginning to end, and that you print out the performance of the agent at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "D4dEq-Lc327u",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "YP2607\n",
    "\n",
    "Utilizing my late days for the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7-Vo-FNPRX9V",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hZNKbkOPTeLs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gamma = 0.99  # discount \n",
    "envname = \"CartPole-v0\"  # environment name\n",
    "env = gym.make(envname)\n",
    "obssize = env.observation_space.low.size\n",
    "actsize = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GujIeXW2RulT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## DQN (Deep Q Network)\n",
    "\n",
    "In previous HWs, we have learned to use Tensorflow to build deep learning models. In this HW, we will apply deep learning as function approximations in reinforcement learning. \n",
    "\n",
    "Reference: DQN https://arxiv.org/abs/1312.5602\n",
    "\n",
    "\n",
    "In tabular Q-learning, we maintain a table of state-action pairs $(s,a)$ and save one action value for each entry $Q(s,a),\\forall (s,a)$. At each time step $t$, we are in state $s_t$, then we choose action based on $\\epsilon-$greedy strategy. With prob $\\epsilon$, choose action uniformly random; with prob $1-\\epsilon$, choose action based on $$a_t = \\arg\\max_a Q(s_t,a)$$ \n",
    "\n",
    "We then get the instant reward $r_t$, update the Q-table using the following rule\n",
    "\n",
    "$$Q(s_t,a_t) \\leftarrow (1-\\alpha)Q(s_t,a_t) + \\alpha (r_t + \\max_a \\gamma Q(s_{t+1},a))$$\n",
    "\n",
    "where $\\alpha \\in (0,1)$ is learning rate. The algorithm is shown to converge in tabular cases. However, in cases where we cannot keep a table for state and action, we need function approximation. Consider using neural network with parameter $\\theta$, the network takes as input state $s$ and action $a$. (*or some features of state and action*). Let $Q_\\theta(s,a)$ be the output of the network, which estimates the optimal Q-value function for state $s$ and action $a$.\n",
    "$$Q_\\theta(s,a) \\approx Q^\\ast(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "494xoDa8SLHG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def model_creator():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(32,activation=\"relu\",kernel_initializer='he_uniform'))\n",
    "    # you should later modify this neural network\n",
    "    model.add(layers.Dense(32,activation=\"relu\",kernel_initializer='he_uniform'))\n",
    "    model.add(layers.Dense(32,activation=\"relu\",kernel_initializer='he_uniform'))\n",
    "    model.add(layers.Dense(32,activation=\"relu\",kernel_initializer='he_uniform'))\n",
    "    model.add(layers.Dense(actsize,activation=\"linear\")) # you should have one output for each possible action\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9BlBZLeSh2f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We wish to train the neural network in order to find $\\theta$ such that $Q_\\theta(s,a)$ approximates $Q^*(s,a)$. As we discussed in the class, we can use observations of form $(s_i, a_i, r_i, s'_{i})$ (i.e., observing reward $r_i$ and new state $s'_{i}$ on taking action $a_i$ in state $s_i$) for training. Based on observations, we can use stochastic gradient descent to update $\\theta$ in the direction that minimizes the loss function. Further, based on values $Q_\\theta(s,a)$, we can choose the action based on $\\epsilon$-greedy policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHaI6w4BSc3Q",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Formally let $d_i$ be the target for the $i$-th sample $(s_t,a_t,r_t,s_{t+1})$\n",
    "\n",
    "$$d_i =  r_t +   \\gamma \\max_a Q_\\theta(s_{t+1},a)$$\n",
    "\n",
    "We can collect a batch of $N$ samples (this generalizes the per sample update $N=1$ discusssed in class), consider the loss fucntion,\n",
    "\n",
    "$$J:=\\frac{1}{N} \\sum_{i=1}^N (Q_\\theta(s_i,a_i) - d_i)^2$$\n",
    "\n",
    "and update\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta -\\alpha \\nabla_\\theta J\n",
    "$$\n",
    "\n",
    "This procedure has been shown to be fairly unstable. In class, we discussed two techniques to stabilize the training process: target network and replay buffer.\n",
    "\n",
    "**Replay Buffer**\n",
    "Maintain a buffer $R$ to store trainsition tuples $(s_t,a_t,r_t,s_{t+1})$. When minimizing the loss, we sample batches from the replay buffer and compute gradients for update on these batches. In particular, in each update, we sample $N$ tuples $(s_t,a_t,r_t,s_{t+1})$ from buffer $R$ and then minimize the\n",
    "loss \n",
    "\n",
    "$$\\frac{1}{N} \\sum_{i=1}^N (Q_\\theta(s_i,a_i) -  (r_i + \\gamma \\max_a Q_\\theta(s_i^\\prime,a))^2$$\n",
    "\n",
    "and update parameters.\n",
    "\n",
    "**Target Network**\n",
    "Maintain a target network in addition to the original principal network. The target network is just a copy of the original network but the parameters are not updated by gradients. The target network $\\theta^{-}$ is copied from the principal network every $\\tau$ time steps. Target network is used to compute the targets for update\n",
    "\n",
    "$$d_i = \\max_a r_t + \\gamma Q_{\\theta^{-}}(s_{i}^\\prime,a)$$\n",
    "\n",
    "the targets are used in the loss function to update the principal network parameters. This slowly updated target network ensures that the targets come from a relatively stationary distribution and hence stabilize learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WKt2Mt6TTZsk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# Model used for selecting actions (principal)\n",
    "model = model_creator()\n",
    "# Then create the target model. This will periodically be copied from the principal network \n",
    "model_target = model_creator()\n",
    "\n",
    "model.build((batch_size,obssize,))\n",
    "model_target.build((batch_size,obssize,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JA7a0AXiVO7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Complete the code below to learn an Agent using DQN. \n",
    "- You should tweak the Neural network appropriately to achieve a good reward (>100). Ideally you would want to have a reward close to 200.\n",
    "- The reference paper performs updates every 4 actions. You can experiment with this parameter to speed up the learning\n",
    "- You can experiment with other parameters as well, like learning rate, memory size, different exploration schemes (e.g., adaptive $\\epsilon$ or strategic explorations with bonus rewards) and others.\n",
    "\n",
    "- As we mentioned in class, there are multiple ways to improve the efficiency even further. OPTIONALLY you can experiment with these:\n",
    "  - Prioritized Replay buffer.\n",
    "  - Double DQN \n",
    "  -Dueling DQN architecture.\n",
    "\n",
    "- In case you need to debug your code you can try printing relevant information as the training happens. For example although the performance might vary from iteration to iteration, the average Q values might increase overtime in a more smooth way. This is discussed in the refernece paper\n",
    "\n",
    "- Create a plot of the running reward sampled throughout the training at the frequency of your choice at the end of the training\n",
    "- OPTIONALLY you can create a plot for the average Q-values of the principal Q-network sampled at the frequency of your choice\n",
    "\n",
    "- Ideally you want to learn with as few episodes as possible. However you will not be graded on sample efficiency in this homework. You encouraged to try to learn efficiently though.\n",
    "\n",
    "- Note that the skeleton code includes the GradientTape construct to do the learning. Take a look [here](https://www.tensorflow.org/api_docs/python/tf/GradientTape) for an explanation of GradientTape. It allows for more flexibility than model.fit. Also it uses Adam (Adaptive Moment Estimation) for Stochastic Gradient Descent optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kn81y4Iz_QlJ",
    "outputId": "001f512b-15ee-419e-d2be-3893e77436c7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "running reward: 12.76 at episode 49, frame count 1000, epsilon 0.9099999999999655\n",
      "running reward: 13.05 at episode 97, frame count 2000, epsilon 0.819999999999931\n",
      "running reward: 13.74 at episode 138, frame count 3000, epsilon 0.7299999999998965\n",
      "running reward: 13.73 at episode 188, frame count 4000, epsilon 0.639999999999862\n",
      "running reward: 13.92 at episode 232, frame count 5000, epsilon 0.5499999999998275\n",
      "running reward: 15.99 at episode 275, frame count 6000, epsilon 0.4599999999998177\n",
      "running reward: 21.71 at episode 285, frame count 7000, epsilon 0.36999999999983874\n",
      "running reward: 28.39 at episode 293, frame count 8000, epsilon 0.27999999999985975\n",
      "running reward: 32.14 at episode 301, frame count 9000, epsilon 0.18999999999986225\n",
      "running reward: 35.85 at episode 309, frame count 10000, epsilon 0.1\n",
      "running reward: 40.68 at episode 315, frame count 11000, epsilon 0.1\n",
      "running reward: 44.51 at episode 321, frame count 12000, epsilon 0.1\n",
      "running reward: 48.19 at episode 326, frame count 13000, epsilon 0.1\n",
      "running reward: 51.74 at episode 331, frame count 14000, epsilon 0.1\n",
      "running reward: 53.13 at episode 338, frame count 15000, epsilon 0.1\n",
      "running reward: 54.82 at episode 345, frame count 16000, epsilon 0.1\n",
      "running reward: 56.27 at episode 352, frame count 17000, epsilon 0.1\n",
      "running reward: 58.17 at episode 358, frame count 18000, epsilon 0.1\n",
      "running reward: 59.43 at episode 364, frame count 19000, epsilon 0.1\n",
      "running reward: 59.99 at episode 373, frame count 20000, epsilon 0.1\n",
      "running reward: 60.89 at episode 389, frame count 22000, epsilon 0.1\n",
      "running reward: 61.38 at episode 396, frame count 23000, epsilon 0.1\n",
      "running reward: 62.41 at episode 402, frame count 24000, epsilon 0.1\n",
      "running reward: 63.97 at episode 407, frame count 25000, epsilon 0.1\n",
      "running reward: 65.21 at episode 412, frame count 26000, epsilon 0.1\n",
      "running reward: 65.92 at episode 418, frame count 27000, epsilon 0.1\n",
      "running reward: 66.69 at episode 424, frame count 28000, epsilon 0.1\n",
      "running reward: 67.18 at episode 431, frame count 29000, epsilon 0.1\n",
      "running reward: 67.80 at episode 436, frame count 30000, epsilon 0.1\n",
      "running reward: 68.63 at episode 442, frame count 31000, epsilon 0.1\n",
      "running reward: 69.27 at episode 448, frame count 32000, epsilon 0.1\n",
      "running reward: 69.49 at episode 455, frame count 33000, epsilon 0.1\n",
      "running reward: 69.77 at episode 461, frame count 34000, epsilon 0.1\n",
      "running reward: 70.11 at episode 468, frame count 35000, epsilon 0.1\n",
      "running reward: 70.04 at episode 475, frame count 36000, epsilon 0.1\n",
      "running reward: 70.14 at episode 482, frame count 37000, epsilon 0.1\n",
      "running reward: 70.13 at episode 490, frame count 38000, epsilon 0.1\n",
      "running reward: 70.23 at episode 497, frame count 39000, epsilon 0.1\n"
     ]
    }
   ],
   "source": [
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     0.01,\n",
    "#     decay_steps=1,\n",
    "#     decay_rate=0.5)\n",
    "tf.random.set_seed(42)\n",
    "lr_schedule=0.0005\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)#0.0005)\n",
    "\n",
    "# Our Experience Replay memory \n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "\n",
    "# Replay memory size\n",
    "max_memory = 1000 # You can experiment with different sizes.\n",
    "\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "timestep_count = 0\n",
    "\n",
    "\n",
    "# how often to train your model - this allows you to speed up learning\n",
    "# by not performing in every iteration learning. See also refernece paper\n",
    "# you can set this value to other values like 1 as well to learn every time \n",
    "\n",
    "update_after_actions = 4\n",
    "\n",
    "# How often to update the target network\n",
    "target_update_every = 1000\n",
    "loss_function = keras.losses.Huber() # You can use the Huber loss function or the mean squared error function \n",
    "max_steps_per_episode = 1000\n",
    "\n",
    "max_episodes = 500\n",
    "max_steps_per_episode = 1000\n",
    "last_n_reward = 100\n",
    "\n",
    "epsilon_max = 1.0\n",
    "epsilon_min = 0.1\n",
    "eps_interval = epsilon_max - epsilon_min\n",
    "epsilon = 1.0\n",
    "penalty = 1.0\n",
    "\n",
    "plot_x = []\n",
    "plot_y = []\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    state = np.array(env.reset())\n",
    "    episode_reward = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        timestep_count += 1\n",
    "\n",
    "        # exploration\n",
    "        prob = np.random.random()\n",
    "        if timestep_count<5000 or prob < epsilon:\n",
    "            action = np.random.choice(actsize)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_t = tf.convert_to_tensor(state)\n",
    "            state_t = tf.expand_dims(state_t, 0)\n",
    "            action_vals = model(state_t, training=False)\n",
    "            # Choose the best action\n",
    "            action = tf.argmax(action_vals[0]).numpy()\n",
    "\n",
    "        epsilon -= eps_interval / 10000\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # follow selected action\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save action/states and other information in replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        rewards_history.append(reward)\n",
    "        done_history.append(done)\n",
    "        episode_reward_history.append(episode_reward)\n",
    "\n",
    "        state = state_next\n",
    "\n",
    "        # Update every Xth frame to speed up (optional)\n",
    "        # and if you have sufficient history\n",
    "        if timestep_count % update_after_actions == 0 and len(action_history) > batch_size:\n",
    "\n",
    "            #  Sample a set of batch_size memories from the history\n",
    "            sample_indices = random.choices(range(len(action_history)), k=batch_size)\n",
    "            state_sample = np.array([state_history[i] for i in sample_indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in sample_indices])\n",
    "            rewards_sample = np.array([rewards_history[i] for i in sample_indices])\n",
    "            action_sample = np.array([action_history[i] for i in sample_indices])\n",
    "            done_sample = np.array([done_history[i] for i in sample_indices])\n",
    "\n",
    "            # Create for the sample states the targets (r+gamma * max Q(...) )\n",
    "            Q_next_state = tf.reduce_max(model_target.predict(state_next_sample), axis=1)\n",
    "            Q_targets = rewards_sample + gamma * Q_next_state\n",
    "            \n",
    "\n",
    "            # If the episode was ended (done_sample value is 1)\n",
    "            # you can penalize the Q value of the target by some value `penalty`\n",
    "            Q_targets = Q_targets * (1 - done_sample) - penalty*done_sample\n",
    "\n",
    "            # What actions are relevant and need updating\n",
    "            relevant_actions = tf.one_hot(action_sample, actsize)\n",
    "            # we will use Gradient tape to do a custom gradient \n",
    "            # in the `with` environment we will record a set of operations\n",
    "            # and then we will take gradients with respect to the trainable parameters\n",
    "            # in the neural network\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on your action selecting network\n",
    "                q_values = model(state_sample) \n",
    "                # We consider only the relevant actions\n",
    "                Q_of_actions = tf.reduce_sum(tf.multiply(q_values, relevant_actions), axis=1)\n",
    "                # Calculate loss between principal network and target network\n",
    "                loss = loss_function(Q_targets, Q_of_actions)\n",
    "\n",
    "            # Nudge the weights of the trainable variables towards \n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if timestep_count % target_update_every == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}, epsilon {}\"\n",
    "            print(template.format(running_reward, episode_count, timestep_count,epsilon))\n",
    "            plot_x.append(running_reward)\n",
    "            plot_y.append(timestep_count)\n",
    "\n",
    "        # Don't let the memory grow beyond the limit\n",
    "        if len(rewards_history) > max_memory:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "        if done: break\n",
    "\n",
    "    # reward of last 100\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > last_n_reward: del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    #epsilon = max(0.01, eps_interval*np.exp(-0.005*episode))\n",
    "    #print(epsilon)\n",
    "    episode_count += 1\n",
    "\n",
    "    # If you want to stop your training once you achieve the reward you want you can\n",
    "    # have an if statement here. Alternatively you can stop after a fixed number\n",
    "    # of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "StJsm3T3UU_6",
    "outputId": "4fb5348e-6831-46e5-88ce-b7b7c2c400bf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(plot_y, plot_x)\n",
    "plt.xlabel('Timestep count')\n",
    "plt.ylabel('Running reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-kUmOPTiMXX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Evaluate the performance of the agent on 100 episodes on the environment and print out the average testing performance. Alternatively you can make sure the code above terminates with 100 episodes where there is no exploration at all (epsilon=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xe2V09jdiNXI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b22ae71d-a4e6-41c0-f95b-ea76982ef750",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "119.01\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE Here\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "timestep_count = 0\n",
    "\n",
    "\n",
    "max_episodes = 100\n",
    "max_steps_per_episode = 1000\n",
    "\n",
    "episode_reward_history = []\n",
    "epsilon = 0.0\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    state = np.array(env.reset())\n",
    "    episode_reward = 0\n",
    "\n",
    "    frame_count = 0\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        timestep_count += 1\n",
    "\n",
    "        prob = np.random.random()\n",
    "        if prob < epsilon:\n",
    "            action = np.random.choice(actsize)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_t = tf.convert_to_tensor(state)\n",
    "            state_t = tf.expand_dims(state_t, 0)\n",
    "            action_vals = model(state_t, training=False)\n",
    "            # Choose the best action\n",
    "            action = tf.argmax(action_vals[0]).numpy()\n",
    "\n",
    "        # follow selected action\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "        episode_reward += reward\n",
    "\n",
    "        state = state_next\n",
    "        if done:\n",
    "          break\n",
    "\n",
    "    episode_reward_history.append(episode_reward)\n",
    "\n",
    "print(sum(episode_reward_history)/len(episode_reward_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1AyntwDsnFbl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW4_DQN_yp2607.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}