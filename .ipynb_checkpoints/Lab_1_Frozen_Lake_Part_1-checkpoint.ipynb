{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "auburn-pittsburgh",
   "metadata": {
    "id": "auburn-pittsburgh",
    "tags": []
   },
   "source": [
    "# Lab 1 - FrozenLake MDP: Part 1\n",
    "# Assignment\n",
    "\n",
    "- In this assignment you will learn how to tackle problems with limited state spaces.\n",
    "- In particular we consider the FrozenLake MDP problem.\n",
    "\n",
    "# Outline\n",
    "\n",
    "- Part 0 introduces us to [gym](https://gym.openai.com/), an environment that allows us to test our reinforcement learning algorithms in various problems\n",
    "- In Part 1, you will implement a policy iteration algorithm (HW1)\n",
    "- In Part 2, you will implement Q-Learning and SARSA (in next homework HW2) \n",
    "\n",
    "# Deliverable\n",
    "\n",
    "Regarding the Lab:\n",
    "\n",
    "- Make sure your code runs from top to bottom without any errors.\n",
    "- Your submitted Notebook must contain saved outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "flush-grove",
   "metadata": {
    "id": "flush-grove"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# You will need numpy and gym. You can try running the following lines to install them\n",
    "# The assignment is tested on Python3.8 so in case you are having installation issues you might \n",
    "# want to try installing that version. \n",
    "\n",
    "# !{os.sys.executable} -m pip install numpy\n",
    "# !{os.sys.executable} -m pip install gym\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-dancing",
   "metadata": {
    "id": "medium-dancing"
   },
   "source": [
    "# Part 0 - Introduction to Gym\n",
    "- We look at [FrozenLake-v0 environment](https://gym.openai.com/envs/FrozenLake-v0/) in gym. \n",
    "- You don't need to write any code for this part\n",
    "- you should still understand the code to help you solve Part 1 and Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "considerable-swing",
   "metadata": {
    "id": "considerable-swing"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of States 16, Number of Actions 4\n",
      "Reward range (0, 1)\n"
     ]
    }
   ],
   "source": [
    "# Import the environment we will use in this assignment\n",
    "# env=gym.make('FrozenLake-v0') \n",
    "env=gym.make('FrozenLake-v1',is_slippery=True)\n",
    "# Show the model\n",
    "print(f\"Number of States {env.nS}, Number of Actions {env.nA}\")\n",
    "print(f\"Reward range {env.reward_range}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "severe-conjunction",
   "metadata": {
    "id": "severe-conjunction"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset() # reset the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "endless-cassette",
   "metadata": {
    "id": "endless-cassette"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# visualize the current state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "naughty-bread",
   "metadata": {
    "id": "naughty-bread"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Render State after 25 slots\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reached terminal state? True\n"
     ]
    }
   ],
   "source": [
    "# run a policy that chooses actions randomly \n",
    "env.reset()\n",
    "n = 25\n",
    "for i in range(n):\n",
    "    a = env.action_space.sample() # Sample Random Action\n",
    "    state, reward, finished, _ = env.step(a)\n",
    "    if finished: break\n",
    "        \n",
    "print(f'Render State after {n} slots')\n",
    "env.render()\n",
    "print(f'Reached terminal state? {finished}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "front-nomination",
   "metadata": {
    "id": "front-nomination"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset() # Let's reset the state again\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-china",
   "metadata": {
    "id": "official-china"
   },
   "source": [
    "# Part 1 - MDP and Planning: Implement Policy Iteration \n",
    "- In this part we will focus on methods that assume knowledge of the enivonment dynamics, in partucular you will implement Policy Iteration. \n",
    "- The environment model can be obtained through `env.P`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "spanish-feeding",
   "metadata": {
    "id": "spanish-feeding"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 13, 0.0, False),\n",
       " (0.3333333333333333, 14, 0.0, False),\n",
       " (0.3333333333333333, 9, 0.0, False)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No need to change anything here. Try to understand what happens \n",
    "\n",
    "# let's look at a random state-action pair and observe its transition characteristics\n",
    "# you can re-run this cell to get a different state-action pair\n",
    "random_state  = env.observation_space.sample()\n",
    "random_action = env.action_space.sample()\n",
    "# returns a list of tuples (probability,newstate,reward,is_terminal_state)\n",
    "env.P[random_state][random_action] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lucky-place",
   "metadata": {
    "id": "lucky-place"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5, 7, 11, 12, 15}\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# YOUR CODE HERE #\n",
    "# Print all the terminal states in the environment.\n",
    "# you can use env.P\n",
    "\n",
    "terminal_states = set()\n",
    "for state in range(env.nS):\n",
    "    for action in range(env.nA):\n",
    "        for possible_state in env.P[state][action]:\n",
    "            if possible_state[3]:\n",
    "                terminal_states.add(possible_state[1])\n",
    "\n",
    "print(terminal_states)\n",
    "        \n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rocky-danger",
   "metadata": {
    "id": "rocky-danger"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Verify your solution (look at the positions where final states are)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-pendant",
   "metadata": {
    "id": "protective-pendant"
   },
   "source": [
    "\n",
    "### Step A: Implement Policy Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "forty-offering",
   "metadata": {
    "id": "forty-offering"
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
    "    \"\"\"Evaluate the value function from a given policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    policy: np.array[nS]\n",
    "        The policy to evaluate. Maps states to actions.\n",
    "    tol: float\n",
    "        Terminate policy evaluation when\n",
    "            max |value_function(s) - prev_value_function(s)| < tol\n",
    "    Returns\n",
    "    -------\n",
    "    value_function: np.ndarray[nS]\n",
    "        The value function of the given policy, where value_function[s] is\n",
    "        the value of state s\n",
    "    \"\"\"\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        diff = 0\n",
    "        for s in range(nS):\n",
    "                        \n",
    "            v = value_function[s] \n",
    "            value_function[s] = sum([p*(r+gamma*value_function[n]) for p,n,r,_ in env.P[s][policy[s]]])\n",
    "            diff = max(diff,abs(v-value_function[s]))\n",
    "\n",
    "        if diff < tol:\n",
    "            break\n",
    "\n",
    "    ############################\n",
    "    return value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-property",
   "metadata": {
    "id": "stunning-property"
   },
   "source": [
    "#### Evaluate random policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "naked-tooth",
   "metadata": {
    "id": "naked-tooth"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Policy 0 ------------------------------\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "-------- Policy 1 ------------------------------\n",
      "[0.         0.         0.         0.         0.0059607  0.\n",
      " 0.         0.         0.02012783 0.06142495 0.18492818 0.\n",
      " 0.         0.23755788 0.55531248 0.        ]\n",
      "-------- Policy 2 ------------------------------\n",
      "[0.00922409 0.00430773 0.00157418 0.00063197 0.01432406 0.\n",
      " 0.00047226 0.         0.03407256 0.06559809 0.0198211  0.\n",
      " 0.         0.19902548 0.39898731 0.        ]\n",
      "-------- Policy 3 ------------------------------\n",
      "[0.00726353 0.00336712 0.00152839 0.00079004 0.0155139  0.\n",
      " 0.04890024 0.         0.04483524 0.10520514 0.16312308 0.\n",
      " 0.         0.18786644 0.43863019 0.        ]\n",
      "-------- Policy 4 ------------------------------\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Test your policy_evaluation on 5 randomly generated deterministic policies\n",
    "# print the value function of the policies\n",
    "\n",
    "############################\n",
    "# YOUR CODE HERE #\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    random_policy = np.random.randint(env.nA,size=env.nS)\n",
    "    print(f'-------- Policy {i}','-'*30)\n",
    "    print(policy_evaluation(env.P, env.nS, env.nA, random_policy))\n",
    "\n",
    "############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-evaluation",
   "metadata": {
    "id": "ongoing-evaluation"
   },
   "source": [
    "### Step B: Implement Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cooperative-dressing",
   "metadata": {
    "id": "cooperative-dressing"
   },
   "outputs": [],
   "source": [
    "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Given the value function from policy improve the policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    value_from_policy: np.ndarray\n",
    "        The value calculated from the policy\n",
    "    policy: np.array\n",
    "        The previous policy.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_policy: np.ndarray[nS]\n",
    "        An array of integers. Each integer is the optimal action to take\n",
    "        in that state according to the environment dynamics and the\n",
    "        given value function.\n",
    "    \"\"\"\n",
    "    new_policy = np.zeros(nS, dtype='int')\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    \n",
    "    for s in range(nS):\n",
    "        new_policy[s] = np.argmax([sum([p*(r+gamma*value_from_policy[n]) for p,n,r,_ in env.P[s][a]])\n",
    "                                 for a in range(nA)])\n",
    "            \n",
    "    ############################\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "behavioral-penetration",
   "metadata": {
    "id": "behavioral-penetration"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Policy 0 ------------------------------\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.14251    0.\n",
      " 0.         0.         0.47584333 0.        ]\n",
      "-------> Policy 0 IMPROVED ---------------------\n",
      "[0.         0.         0.03494066 0.01474878 0.         0.\n",
      " 0.08308461 0.         0.         0.14678545 0.2427983  0.\n",
      " 0.         0.24802032 0.5800101  0.        ]\n",
      "-------- Policy 1 ------------------------------\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "-------> Policy 1 IMPROVED ---------------------\n",
      "[0.         0.         0.02279889 0.00950109 0.         0.\n",
      " 0.05453427 0.         0.         0.         0.15918618 0.\n",
      " 0.         0.         0.47615923 0.        ]\n",
      "-------- Policy 2 ------------------------------\n",
      "[0.00483051 0.00936909 0.0265529  0.         0.00239941 0.\n",
      " 0.0793711  0.         0.0009961  0.17325761 0.23839423 0.\n",
      " 0.         0.3402466  0.62182426 0.        ]\n",
      "-------> Policy 2 IMPROVED ---------------------\n",
      "[0.02581715 0.03683704 0.0612966  0.02617257 0.06662355 0.\n",
      " 0.10691814 0.         0.13116705 0.24057771 0.2953762  0.\n",
      " 0.         0.37599919 0.63726053 0.        ]\n",
      "-------- Policy 3 ------------------------------\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "-------> Policy 3 IMPROVED ---------------------\n",
      "[0.         0.         0.02279889 0.00950109 0.         0.\n",
      " 0.05453427 0.         0.         0.         0.15918618 0.\n",
      " 0.         0.         0.47615923 0.        ]\n",
      "-------- Policy 4 ------------------------------\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "-------> Policy 4 IMPROVED ---------------------\n",
      "[0.         0.         0.02279889 0.00950109 0.         0.\n",
      " 0.05453427 0.         0.         0.         0.15918618 0.\n",
      " 0.         0.         0.47615923 0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Print the value before and after policy improvements for 5 randomly generated policies\n",
    "\n",
    "############################\n",
    "# YOUR CODE HERE #\n",
    "\n",
    "for i in range(5):\n",
    "    random_policy = np.random.randint(env.nA,size=env.nS)\n",
    "    print(f'-------- Policy {i}','-'*30)\n",
    "    value_function = policy_evaluation(env.P, env.nS, env.nA, random_policy)\n",
    "    print(value_function)\n",
    "    \n",
    "    print(f'-------> Policy {i} IMPROVED','-'*21)\n",
    "    new_policy = policy_improvement(env.P, env.nS, env.nA, value_function, random_policy)\n",
    "    print(policy_evaluation(env.P, env.nS, env.nA, new_policy))\n",
    "\n",
    "############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-tragedy",
   "metadata": {
    "id": "independent-tragedy"
   },
   "source": [
    "### Step C: Implement Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ranking-template",
   "metadata": {
    "id": "ranking-template"
   },
   "outputs": [],
   "source": [
    "def policy_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "    \"\"\" \n",
    "    Run policy iteration for dynamics of P.\n",
    "\n",
    "    You should use your methods: policy_evaluation() and policy_improvement() here\n",
    "\n",
    "    Parameters: \n",
    "    P, nS, nA, gamma: defined at beginning of file\n",
    "    tolerance:        tolerance parameter used in policy_evaluation()\n",
    "    \n",
    "    Returns: \n",
    "    value_function: np.ndarray[nS]\n",
    "    policy:         np.ndarray[nS]\n",
    "    \"\"\"\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        value_function = policy_evaluation(P, nS, nA, policy, gamma, tol)\n",
    "        new_policy = policy_improvement(P, nS, nA, value_function, policy, gamma)\n",
    "        \n",
    "        if np.array_equal(policy,new_policy):\n",
    "            break\n",
    "            \n",
    "        policy = new_policy\n",
    "        \n",
    "    ############################\n",
    "    return value_function, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-cookie",
   "metadata": {
    "id": "individual-cookie"
   },
   "source": [
    "#### Call your function for gamma=0.9 and gamma=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "subsequent-tonight",
   "metadata": {
    "id": "subsequent-tonight"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.06476289, 0.05841569, 0.07252933, 0.05378964, 0.08867795,\n",
       "        0.        , 0.11137269, 0.        , 0.14325408, 0.24628655,\n",
       "        0.29886952, 0.        , 0.        , 0.37915415, 0.63865133,\n",
       "        0.        ]),\n",
       " array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_pi_s, p_pi_s = policy_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
    "V_pi_s, p_pi_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "military-presence",
   "metadata": {
    "id": "military-presence"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00069547, 0.00139264, 0.00562142, 0.00171616, 0.00280064,\n",
       "        0.        , 0.02169924, 0.        , 0.01214942, 0.04755381,\n",
       "        0.1032758 , 0.        , 0.        , 0.12348492, 0.44745551,\n",
       "        0.        ]),\n",
       " array([1, 3, 2, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_pi_f, p_pi_f = policy_iteration(env.P, env.nS, env.nA, gamma=0.6, tol=1e-3)\n",
    "V_pi_f, p_pi_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-unknown",
   "metadata": {
    "id": "fifth-unknown"
   },
   "source": [
    "#### What do you observe in terms of impact of gamma on the actions taken by the policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GGe3wwuEoKye",
   "metadata": {
    "id": "GGe3wwuEoKye"
   },
   "source": [
    "The optimal policy or the action taken are dependendent on the value of gamma. For gamma=0.6 the value function of each state is considerably low as when compared to that of gamma=0.9. This is because with higer gamma the impact of future reward is more, so the states have higer value function in cases where the actions allow it to reach terminal states. As we discussed in class, when gamma is low, the algorithm gives importance to immediate rewards. The immediate rewards of all the beginning states are low so the action taken is done to maximize the more immediate reward, so we see that value functions are low too. However for higher gamma, the actions are maximizing the future reward which allows it to reach goal states and have higher value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d0044e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_1_Frozen_Lake_Part_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
